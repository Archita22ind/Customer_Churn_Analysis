%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
\usepackage[]{graphicx}  

\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{xcolor} % for pdf, bitmapped graphics files
\usepackage[dvipsnames]{xcolor}

\title{\LARGE \bf
Customer Churn Analysis}



\author{Aishwarya Paruchuri, Archita Chakraborty, Manjushree Rajanna, Rohit Chandra\\San Jose State University\\ November 2021% <-this % stops a space
}

\date{November 2021}

\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\\\
\begin{abstract}


\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\textbf{Introduction}}
The mobile services market is growing significantly and sustainably, not only due to the size of the market, but also due to the increasing variety of services offered and fierce competition in the telecommunication industry. Regardless of the earliest stages of this industry, the method of contest has moved from procuring new endorsers to holding existing customers. This has been accomplished by participating in showcasing endeavors and by luring customers from rival organizations.
\\
\\As indicated by a survey in 2004, it cost around 300 dollars per record to secure new clients and 25 dollars to hold existing clients. This implies that it was significantly more costly to procure new clients than to hold existing ones. Hence, ﬁnancially, it makes more sense for an organization to focus on retaining its existing customers. As a result, churn management is a major area of focus. 

\\\
\subsection{\textbf{Dataset}}
\\The data set has been randomly collected from an Iranian telecommunication company’s database over a period of 12 months. The data set contains 3150 customer data with the below mentioned columns-
\\
\begin{figure}[htp]
    \centering
    \includegraphics[width=9cm,height=5.5cm]{Images/dataset.png}
    \caption{Table : Dataset}
    \label{fig:datasetUsed}
\end{figure}
\\\
\\Note: The output feature - "churn" has  495 records which belongs to the churned class and 2645 records belong to the non-churned class. This shows that the data is highly imbalanced.
\\\
\subsection{\textbf{Association Analysis}}
\\\
\\Multiple factors can affect customer churn(Fig. 1).
\\\
\\1. {Customer dissatisfaction}: 
Customer complaints and Service failure rates are positively associated and length of customer association is negatively associated with customer churn probability. 
\\\
\\2. {Level of service usage}: 
Number of calls, Minutes of monthly use and Number of distinct calls are negatively associated with customer churn probability. 
\\\
\\3. {Switching costs}: 
Type of service is positively associated with the probability of subscriber churn.
\\\
\\4. {Customer demographic variable}: 
Customer age is positively associated with customer churn probability.
\\\
% \usepackage{graphicx}
\begin{figure}[htp]
    \centering
    \includegraphics[scale= .55]{Images/AssociationAnalysis.png}
    \caption{Association Analysis}
    \label{fig:AssociationAnalysis}
\end{figure}
\\
\subsection{\textbf{ Exploratory Data Analysis}}
\\\
\\\textbf{1. Data Preprocessing}
\\It's a data mining approach for converting raw data into a usable and efficient format.
\\\
\\
Steps Involved in Data Preprocessing: 
\\
\\\
(A) Data Cleaning: 
There may be various useless and missing elements in the raw data. Data cleaning is used to deal with this aspect. It entails dealing with missing data and noisy data.
% \\\
% Handling Missing Data: This situation arises when some data is missing in the data.
% \\\
\\ Method used to find the null values: Missingno()
\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm,height=6cm]{Images/missing_Plot.png}
    \caption{Missingo Matrix Plot}
    \label{fig:MissingnoBarPlot}
\end{figure}
\\\
\textit{Observation: Missing values are shown by white striped lines in each column. The columns such as, call failure, complaints, subscription length, charge amount, seconds of use, frequency of use, and customer value have missing values that must be cleaned.}
\\\
\\\
Techniques used to handle null values in the data:
\\
\\i) Median: Some attributes, such as {secondsOfUse} and {customerValue}, have outliers. We impute median value in place of null values for these columns because mean is prone to outliers.
\\ii) Mean : We impute mean values in place of  null values for the remaining features that don't have outliers.
\\\
\\\
(B) Outlier Detection:
\\\
\\Outliers are extreme values that deviate from other observations on data , they may indicate a variability in a measurement, experimental errors or a novelty. In other words, an outlier is an observation that diverges from an overall pattern on a sample.

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm,height=6cm]{Images/boxplot.png}
    \caption{Null Values in each column of the data set}
    \label{fig:boxplot}
\end{figure}
\\\
\\\
\textit{Observation- As we can see in Fig.4, there are outliers in our data set, especially seconds of use feature has the most outliers.}
\\\
\\\
Methods used to treat Outliers:
\\i) Drop the outliers
\\ii) Replace with median or a constant value
\\\
\\\
\\
\\\
\textbf{2. Feature Scaling}: This step is conducted to convert the data into a format that can be used in the mining process. Some of the classification models are based on probability, so we have scaled the data using MinMaxScalar() which transforms the values between 0 to 1 instead of StandardScalar() which transforms the data between -1 to 1.
 

 
\\\
\\\textbf{3. Data Visualizations}
The graphical depiction of information and data is known as data visualisation. Data visualisation tools make it easy to examine and comprehend trends, outliers, and patterns in data by employing visual elements like charts, graphs, and maps.

\\\
\\Distribution of the output feature - "Churn":
\begin{figure}[htp]
    \centering
    \includegraphics[width=6cm]{Images/Churn_distribution.png}
    \caption{Distribution of Churn}
\end{figure}
\\\
\textit{Observation- In the output column, we can see that there are 84.29 percent non-churn customers and 15.71 percent churn customers in total, indicating a data imbalance.}
\\
\\
\\
\\
\\
\\
\\
\\Distribution of all the categorical features:
\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm,height=6cm]{Images/frequencyDistribution.png}
    \caption{Frequency Distribution of Different Categorical Variable}
    \label{fig:frequencyDistribution}
\end{figure}
\\\textit{Observation1- We notice that many of the customers are between the ages of 30 and 60, and they are less likely to churn
\\Observation2- We see a big difference in count between churn and non-churn active customers (where active customers are more likely to not churn) and churn and non-churn inactive customers (where inactive customers are more likely to churn).
 \\Observation3- We notice that a higher percentage of consumers have no call failures and thus no complaints.
\\Observation4- We notice that a higher percentage of consumers in the 30-60 age group have a charge amount of o, which denotes the lowest salary}
\\\
\\Distribution of all the numerical features
\begin{figure}[htp]
    \centering
    \includegraphics[width=9cm,height=6cm]{Images/Histogram.png}
    \caption{Distribution of all columns}
    \label{fig: Destribution of all columns}
\end{figure}
\\\

\\\textit{Observation- Above figure shows the frequency distribution of all the columns are shown above.}
\\
\\
\\
\\
\textbf{4. Feature Engineering}
\\\
\\It involves deriving new features based on the existing features in the data set. In the data set, We have identified Age feature and performed feature engineering on it to create a new feature called AgeGroup to combine different age values in
five different age intervals. The following table shows the age intervals:

\\\
\begin{figure}[htp]
    \centering
    \includegraphics[width=5cm,height=4cm]{Images/agegrouptable.png}
    \caption{Table: Age Group}
    \label{fig:agegrouptable}
\end{figure}

\\\
\begin{figure}[htp]
    \centering
    \includegraphics[width=5cm,height=5.5cm]{Images/distributionOfAge.png}
    \caption{Pie Chart displaying the age distribution of customers}
    \label{fig:distributionOfAge}
\end{figure}
\\\
\\\
\textit{Observation- As we can see from the pie chart, most of the customers belong to the age group 30-45.}

\\\
\\\textbf{5. Correlation analysis}



\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm,height=6cm]{Images/heatmap.png}
    \caption{Heatmap for correlation coefficient}
    \label{fig:heatmap}
\end{figure}
\\\
\\From Fig.10 :
\\\textit{a: We observe a  positive correlation between freq of use and distinct call numbers. The correlation coefficient value is 0.9389.
\\b: We observe a  positive correlation between charge amount and call failure. The correlation coefficient value is 0.5817.
\\c: We observe a  positive correlation between freq of use and customer value. The correlation coefficient value is 0.9249.
\\d: We observe very less correlation between call failure and churn. The correlation coefficient value is -0.0093.}
\\\
\\\
\\\textbf{6. Feature Selection}
\\
\\We used SelectKBest feature selection technique to select top 13 features to train different multi-classification model. We can visualize with the help of horizontal bar plot shown below.
\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm,height=6cm]{Images/featureImportance.png}
    \caption{Churn vs all features}
    \label{fig:featureImportance}
\end{figure}

\\\textit{Observation- From the graph we observe that status and complain are influential features for this dataset.}

\\\
\section{\textbf{METHODS}}
As the data is highly imbalanced, we used Synthetic Minority Oversampling Technique(SMOTE), which involves duplicating samples in the minority class and Under Sampling Majority data set technique(UnderSampling), which randomly adds more minority observations by replication.
All our analysis is done with imbalanced data, SMOTE generated data and under-sampling generated data. While training models on these datasets, we performed hyper-parameter tuning using GridSearchCV, which loops over predefined hyper-parameters and fits the model to the training data using best parameter values obtained.
We considered following models to analyse the data:
\\\
\\1)XGBoost Classifier-
XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. The goal is to improve weak learners by using a gradient descent approach to minimise errors. When compared to other algorithms, it is thought to be exceptionally efficient and quick. 
XGBoost classifier yields following results:

\\\
\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm,height=2cm]{Images/xgbclassifier.png}
    \caption{XGB Classifier Results}
    \label{fig:xgbclassifier}
\end{figure}
\\\
\\2) Naive Bayes Classifier- The Bayes Theorem-based probabilistic machine learning method Naive Bayes(NB) is employed in a wide range of categorization problems.

\\\
\\2a) Gaussian Naive Bayes Classifier- It is a naive bayes algorithm that is unique. When the features have continuous values, it's employed particularly. It's also expected that all of the characteristics have a Gaussian distribution, or a normal distribution.
Gaussian Naive Bayes Classifier yields following results:

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm,height=2cm]{Images/GaussianNB.png}
    \caption{GaussianNB Classifier Results}
    \label{fig:GaussianNB}
\end{figure}
\\\
\\2b) Multinomial Naive Bayes Classifier - It is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. 
Multinomial Naive Bayes Classifier yields following results:

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm,height=2cm]{Images/MultinomialNB.png}
    \caption{Multinomial Classifier Results}
    \label{fig:MultinomialNB}
\end{figure}
\\\
\\2c) Complement Naive Bayes Classifier - It is well-suited to dealing with unbalanced data sets. Instead of calculating the probability of an item belonging to a certain class, we calculate the probability of the item belonging to all classes in complement Naive Bayes.
Complement Naive Bayes Classifier yields following results:

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm,height=2cm]{Images/ComplementNB.png}
    \caption{ComplementNB Classifier Results}
    \label{fig:ComplementNB}
\end{figure}
\\\
\\\
\\3) Support Vector Classifier(SVC) - It is a linear model that can be used to solve classification and regression problems. It can solve both linear and nonlinear problems.The algorithm generates a line or hyper-plane that divides the data into categories.
 Support Vector Classifier yields following results:
\\\
\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm,height=2cm]{Images/svm.png}
    \caption{SVM Classifier Results}
    \label{fig:svm}
\end{figure}
\\\
\\4) Decision Tree - It is supervised machine learning that categorises or predicts outcomes based on the answers to a previous set of questions.
Decision Tree yields following results:

\\\
\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm,height=2cm]{Images/decisionTree.png}
    \caption{Decision Tree Classifier Results}
    \label{fig:decisionTree}
\end{figure}
\\
\section{\textbf{COMPARISONS}}
\\\\\subsection{\textbf{Performance Metrics}}
\\\
\\1. Precision- The number of positive class predictions that actually belong to the positive class is measured by precision.
2. Recall- The number of positive class predictions made out of all positive examples in the dataset is measured by recall.
3. F1-Score- F1-Score generates a single score that accounts for both precision and recall concerns in a single number.
4. Accuracy- It's the proportion of correct predictions to total input samples. It only works if each class has an equal amount of samples. 
5. AUC - The Area Under the Curve (AUC) is a  curve that measures a classifier's ability to distinguish between classes. The greater the AUC, the better.
\\Note: Since our data is class imbalanced, we majorly rely on F1-Score, Recall and Precision.
\subsection{\textbf{Comparison}}
Among all the models built on imbalanced data, XGBoost is the winner as XGBoost can offer better performance on binary classification problems with a severe class imbalance.The model performed better with good precision(84.32 percent) as well as recall score(76.35 percent). The Fig.18 depicts the high AUC percentage(98 percentage) of XGBoost Classifier wrt other models. 
\\\
\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm,height=6cm]{Images/Roc_without_sampling_final.png}
    \caption{Roc Curve of Models built on Imbalanced data}
    \label{fig:Roc Curve of Models built on Imbalanced data}
\end{figure}
\\\

Among all the models built on Undersampling  generated data, XGBClassifier is the best one. As we can see from the Fig.19, XGBClassifier has highest area under the curve(96 percent).

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm,height=6cm]{Images/Roc_with_undersampling_final.png}
    \caption{Roc Curve of Models built on Undersampling data}
    \label{fig:Roc Curve of Models built on Undersampling data}
\end{figure}
\\\
As we can see from the Fig.20, XGBClassifier has highest area under the curve(98 percent) for balanced data generated using SMOTE technique.
\\\

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm,height=6cm]{Images/Roc_with_SMOTE_final.png}
    \caption{ROC Curve of Models built on SMOTE data}
    \label{fig:Roc Curve of Models built on SMOTE data}
\end{figure}
\\\

The Fig.21 shows the ROC graph of all types of data discussed, built on various models specified above. 

\\\
\begin{figure}[htp]
    \centering
    \includegraphics[scale= .5]{Images/performanceAnalysis.png}
    \caption{Performance Analysis of all Classification Models}
    \label{fig:PerformanceAnalysis}
\end{figure}
\\\
\\\
\\\
\section{\textbf{CONCLUSIONS}}
\\\
\section{\textbf{REFERENCES}}

\\\
\color{teal}
\\\
[1]\textit{\url{ https://analyticsindiamag.com/tips-for-automating-eda-using-pandas-profiling-sweetviz-and-autoviz-in-python/}}
\\\
\\\
[2] \textbf{ Dataset Link:}  \textit{ \url{https://tinyurl.com/TelecomCustomerChurnDataset}}
\\\
\\\
[3]  \textit{Ahmed U, Khan A, Khan SH, Basit A, Haq IU, Lee YS (2019) Transfer learning and meta classification based deep churn prediction system for telecom industry.}
\\\
\\\
[4] \textit{Amin A, Anwar S, Adnan A, Nawaz M, Howard N, Qadir J, Hawalah A, Hussain A (2016) Comparing oversampling techniques to handle the class imbalance problem: a customer churn prediction case study. IEEE Access 4:7940–7957}



\end{document}
